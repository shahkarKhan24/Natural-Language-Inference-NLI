{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahkarKhan24/Natural-Language-Inference-NLI/blob/main/Data_augmentation_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVkLqlCdBYb3",
        "outputId": "3b4c2e29-31e8-4009-fb1b-cc2449d283c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in c:\\users\\shahk\\appdata\\roaming\\python\\python39\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.9.24)\n",
            "Requirement already satisfied: hstspreload in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Requirement already satisfied: chardet==3.* in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.10.1)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\shahk\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (1.23.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.5.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Collecting tqdm>=4.66.3 (from datasets)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: xxhash in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.12.2)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (22.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (5.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shahk\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shahk\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.32.2->datasets) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahk\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.32.2->datasets) (2022.9.24)\n",
            "Requirement already satisfied: colorama in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shahk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "   ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n",
            "   -- ------------------------------------ 41.0/547.8 kB 991.0 kB/s eta 0:00:01\n",
            "   -------------------------------- ------- 450.6/547.8 kB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 547.8/547.8 kB 4.9 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
            "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
            "   --------------------------------------  399.4/402.6 kB 25.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 402.6/402.6 kB 8.3 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
            "Installing collected packages: tqdm, huggingface-hub, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.10.1\n",
            "    Uninstalling huggingface-hub-0.10.1:\n",
            "      Successfully uninstalled huggingface-hub-0.10.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.10.1\n",
            "    Uninstalling datasets-2.10.1:\n",
            "      Successfully uninstalled datasets-2.10.1\n",
            "Successfully installed datasets-2.20.0 huggingface-hub-0.23.4 tqdm-4.66.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "allennlp 2.10.1 requires spacy<3.4,>=2.1.0, but you have spacy 3.7.5 which is incompatible.\n",
            "cached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 0.23.4 which is incompatible.\n",
            "openai 1.12.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.12.0 requires pydantic<3,>=1.9.0, but you have pydantic 1.8.2 which is incompatible.\n",
            "openai 1.12.0 requires typing-extensions<5,>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sentence-transformers 2.3.1 requires transformers<5.0.0,>=4.32.0, but you have transformers 4.20.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUgr5xpBYb-"
      },
      "source": [
        "# Generating Adversarial Dataset:\n",
        "In this task also have to deal with Adversarial-NLI, so modifying the data in a way that the model is more robust for NLI task, so we are asked to generate an adversarial data using original dataset. In order to do that, I used several techniques all combined together to generate my own adversarial dataset comprising of 15000 samples. The focus was on augmenting the hypothesis of each sample, as this is less complex than modifying the premise.\n",
        "The data augmentation techniques utilized include antonym substitution, synonym substitution, and hypernym substitution, facilitated by the Word Sense Disambiguation (WSD) annotations provided in the original dataset. For antonym substitution, the label was also switched (e.g., contradiction to entailment and vice versa). Additionally, I used cross-lingual augmentation technique, involving back-translation, which proved to be the most sophisticated method in preserving the original sentence's semantics, but at the cost of increased processing time. The other technique that I used were very generic because using semantic alone may or may not be beneficial, so I used some non-semantic augmentation but in a very small percentage. One such technique was word-swap, where subjects or objects within a sentence were exchanged, resulting in slight disorganization without losing semantic meaning. And finally, I used Noise insertion technique which simply find a random words or letter in each sentence and add it again in a sentence. This technique can help the model to be more robust to any typos that may be present in a data.\n",
        "We also notice that there were many cases where some technique might not work for a sentence, in that case I simply used again the synonym substitution technique which was way more reliable if any technique fails. Each technique was assigned a specific percentage to determine its application across the subset of 15,000 samples, as detailed in the accompanying table. The techniques assigned a lower percentage were chosen as such due to their higher likelihood of causing a loss of semantic meaning in the sentence. The augmenting took around 3 hours to process for 15000 samples, which were than combine and shuffled across the original training dataset  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKSyyup4BYcD",
        "outputId": "f619ac86-a743-4981-9ebd-9d7e88d6d781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\shahk\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\shahk\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import random\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from googletrans import Translator\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnl2zdvYBYcF"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92eLaqmBYcH"
      },
      "source": [
        "this cell was run to clear the cache to redownlad the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnphMik9BYcI"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# # Path to the cache directory\n",
        "# cache_dir = \"C:/Users/shahk/.cache/huggingface/datasets/tommasobonomo___parquet/tommasobonomo--sem_augmented_fever_nli-7de7767a134493f7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/\"\n",
        "# # Delete the cache directory if it exists\n",
        "# if os.path.exists(cache_dir):\n",
        "#     shutil.rmtree(cache_dir)\n",
        "#     # Set an environment variable to disable caching\n",
        "# os.environ['HF_DATASETS_CACHE'] = '/path/to/non_existent_directory'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4Vczpe3BYcJ",
        "outputId": "fd6334fb-307a-4684-e4f3-d55336d5c5fc",
        "colab": {
          "referenced_widgets": [
            "144c97b785b141c082d714c6579f3eb2",
            "3d3e751a0e374d46949982ad91e16e15",
            "d24cf3fa4ce545fd8ac651e9f0d194b3",
            "45f7fdeab4c24eb2bb31277bb3f66a7e",
            "f5ad7961e9d8447681cd6989af18f419",
            "c62eadea1b434b6cb1c11aa5460963bd",
            "355e920ad0074f75bfe41d9470926f33"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "144c97b785b141c082d714c6579f3eb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d3e751a0e374d46949982ad91e16e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/72.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d24cf3fa4ce545fd8ac651e9f0d194b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/3.25M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45f7fdeab4c24eb2bb31277bb3f66a7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/3.25M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5ad7961e9d8447681cd6989af18f419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/51086 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c62eadea1b434b6cb1c11aa5460963bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/2288 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "355e920ad0074f75bfe41d9470926f33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/2287 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 51086\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 2288\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 2287\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\", download_mode=\"force_redownload\")\n",
        "\n",
        "# View the dataset\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbRITBK9BYcL"
      },
      "outputs": [],
      "source": [
        "# Select the first 15000 samples using the indices\n",
        "# Get the indices of the first 15000 samples\n",
        "indices = list(range(15000))\n",
        "subset_dataset = dataset['train'].select(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jumsXAZmBYcM",
        "outputId": "b21c7f0a-142d-4427-b376-a24c7026a566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': Value(dtype='string', id=None), 'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'wsd': {'premise': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}], 'hypothesis': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}]}, 'srl': {'premise': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}, 'hypothesis': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}}}\n",
            "{'id': Value(dtype='string', id=None), 'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'wsd': {'premise': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}], 'hypothesis': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}]}, 'srl': {'premise': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}, 'hypothesis': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(dataset['train'].features)\n",
        "print(subset_dataset.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp6pfSi4BYcN"
      },
      "source": [
        "<h3>Sysnonyms, Hypernyms and Antonyms Substitution Using WSD Annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5NeFhpdBYcO"
      },
      "outputs": [],
      "source": [
        "# Function to map Universal POS tags to WordNet POS tags\n",
        "def map_pos_universal_to_wordnet(universal_pos):\n",
        "    pos_map = {\n",
        "        'ADJ': wn.ADJ,\n",
        "        'ADP': wn.ADV,   # Mapping ADP to ADV (adverb in WordNet)\n",
        "        'ADV': wn.ADV,\n",
        "        'AUX': wn.VERB,  # Mapping AUX to VERB in WordNet\n",
        "        'CCONJ': None,   # WordNet does not directly support coordinating conjunctions\n",
        "        'DET': None,     # WordNet does not directly support determiners\n",
        "        'NOUN': wn.NOUN,\n",
        "        'NUM': None,     # No direct support for numbers in WordNet\n",
        "        'PART': wn.ADV,  # Mapping PART to ADV (adverb in WordNet)\n",
        "        'PRON': wn.NOUN, # Mapping PRON to NOUN in WordNet\n",
        "        'PROPN': wn.NOUN, # Mapping PROPN to NOUN in WordNet\n",
        "        'PUNCT': None,   # WordNet does not directly support punctuation\n",
        "        'SYM': wn.NOUN,  # Mapping SYM to NOUN in WordNet\n",
        "        'VERB': wn.VERB\n",
        "    }\n",
        "    return pos_map.get(universal_pos)\n",
        "# Function to get synonyms using WordNet and POS tag\n",
        "def get_synonyms(word, universal_pos):\n",
        "    wn_pos = map_pos_universal_to_wordnet(universal_pos)\n",
        "\n",
        "    if wn_pos:\n",
        "        synsets = wn.synsets(word, pos=wn_pos)\n",
        "        synonyms = set()\n",
        "        for synset in synsets:\n",
        "            for lemma in synset.lemmas():\n",
        "                synonym = lemma.name().replace('_', ' ')\n",
        "                if synonym.lower() != word.lower():\n",
        "                    synonyms.add(synonym)\n",
        "        return list(synonyms)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Function to get hypernyms using WordNet and Universal POS tag\n",
        "def get_hypernyms(word, universal_pos):\n",
        "    wn_pos = map_pos_universal_to_wordnet(universal_pos)\n",
        "\n",
        "    if wn_pos:\n",
        "        synsets = wn.synsets(word, pos=wn_pos)\n",
        "        hypernyms = set()\n",
        "        for synset in synsets:\n",
        "            for hypernym in synset.hypernyms():\n",
        "                for lemma in hypernym.lemmas():\n",
        "                    hypernym_word = lemma.name().replace('_', ' ')\n",
        "                    if hypernym_word.lower() != word.lower():\n",
        "                        hypernyms.add(hypernym_word)\n",
        "        return list(hypernyms)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "\n",
        "# Function to get antonyms using WordNet and POS tag\n",
        "def get_antonyms(word, universal_pos):\n",
        "    wn_pos = map_pos_universal_to_wordnet(universal_pos)\n",
        "\n",
        "    if wn_pos:\n",
        "        antonyms = set()\n",
        "        for synset in wn.synsets(word, pos=wn_pos):\n",
        "            for lemma in synset.lemmas():\n",
        "                for antonym in lemma.antonyms():\n",
        "                    antonyms.add(antonym.name().replace('_', ' '))\n",
        "        return list(antonyms)\n",
        "    else:\n",
        "        return []\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd8ZSS5WBYcP"
      },
      "outputs": [],
      "source": [
        "# Function to apply synonym substitution on hypothesis\n",
        "\n",
        "def apply_synonym_substitution(augmented_hyp, wsd_annotations):\n",
        "    techniques = [\"synonym\",  \"hypernym\"]\n",
        "    technique = random.choice(techniques)\n",
        "    count=0\n",
        "    to_replace = random.randint(1,len(wsd_annotations))-1 # random index for first wsd annoation of the hypothesis, mean random word\n",
        "    check=wsd_annotations[to_replace]['wnSynsetOffset']\n",
        "    tag= wsd_annotations[to_replace]['pos']\n",
        "\n",
        "    if check == 'O':\n",
        "        count=0\n",
        "        while check in ['O']:\n",
        "            if count==len(wsd_annotations):\n",
        "                break\n",
        "            else:\n",
        "                to_replace = random.randint(0, len(wsd_annotations) - 1)\n",
        "                check = wsd_annotations[to_replace]['wnSynsetOffset']\n",
        "                tag = wsd_annotations[to_replace]['pos']\n",
        "                count+=1\n",
        "    if count==len(wsd_annotations):\n",
        "        return augmented_hyp\n",
        "\n",
        "    # # Check if tag is in the list of tags to avoid\n",
        "    # if tag in ['CCONJ', 'DET', 'NUM', 'PART', 'PUNCT', 'SYM','PROPN']:\n",
        "    #     count=0\n",
        "    # # If the tag matches one of the specified ones, find a new to_replace\n",
        "    #     while tag in ['CCONJ', 'DET', 'NUM', 'PART', 'PUNCT', 'SYM','PROPN']:\n",
        "    #         if count==len(wsd_annotations):\n",
        "    #             break\n",
        "    #         else:\n",
        "    #             to_replace = random.randint(0, len(wsd_annotations) - 1)\n",
        "    #             tag = wsd_annotations[to_replace]['pos']\n",
        "    #             count+=1\n",
        "    # if count==len(wsd_annotations):\n",
        "    #     return augmented_hyp\n",
        "\n",
        "\n",
        "    word=wsd_annotations[to_replace]['text']\n",
        "    if technique == \"synonym\":\n",
        "        synonyms=get_synonyms(word, tag)\n",
        "    elif technique == \"hypernym\":\n",
        "        synonyms=get_hypernyms(word, tag)\n",
        "\n",
        "    if synonyms:\n",
        "        synonym = random.choice(synonyms)\n",
        "        count2=0\n",
        "        while synonym==word and count2!=len(synonyms):\n",
        "            synonyms.remove(word)\n",
        "            synonym = random.choice(synonyms)\n",
        "            count2+=1\n",
        "\n",
        "        augmented_hyp = augmented_hyp.replace(word, synonym)\n",
        "\n",
        "    return augmented_hyp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9BXu3xtBYcQ"
      },
      "outputs": [],
      "source": [
        "# Function to apply antonym substitution on hypothesis\n",
        "def apply_antonym_substitution(augmented_hyp, wsd_annotations, augmented_labels):\n",
        "    count = 0\n",
        "    to_replace = random.randint(0, len(wsd_annotations) - 1)  # Ensure the random index is within range\n",
        "    tag = wsd_annotations[to_replace]['pos']\n",
        "    check=wsd_annotations[to_replace]['wnSynsetOffset']\n",
        "\n",
        "    if check == 'O':\n",
        "        count=0\n",
        "        while check in ['O']:\n",
        "            if count==len(wsd_annotations):\n",
        "                break\n",
        "            else:\n",
        "                to_replace = random.randint(0, len(wsd_annotations) - 1)\n",
        "                check = wsd_annotations[to_replace]['wnSynsetOffset']\n",
        "                tag = wsd_annotations[to_replace]['pos']\n",
        "                count+=1\n",
        "\n",
        "    if count==len(wsd_annotations):\n",
        "        return augmented_hyp, augmented_labels # if sentence is empty send the same sentence back\n",
        "\n",
        "    # # Check if tag is in the list of tags to avoid\n",
        "    # if tag in ['CCONJ', 'DET', 'NUM', 'PART', 'PUNCT', 'SYM','ADV','ADP']:\n",
        "    #     count = 0\n",
        "    #     while tag in ['CCONJ', 'DET', 'NUM', 'PART', 'PUNCT', 'SYM', 'ADV', 'ADP']:\n",
        "    #         if count == len(wsd_annotations):\n",
        "    #             break\n",
        "    #         else:\n",
        "    #             to_replace = random.randint(0, len(wsd_annotations) - 1)\n",
        "    #             tag = wsd_annotations[to_replace]['pos']\n",
        "    #             count += 1\n",
        "\n",
        "    # if count == len(wsd_annotations):\n",
        "\n",
        "    #     return augmented_hyp, augmented_labels\n",
        "\n",
        "    word = wsd_annotations[to_replace]['text']\n",
        "    antonyms = get_antonyms(word, tag)\n",
        "\n",
        "    if antonyms != []:\n",
        "        antonym = random.choice(antonyms)\n",
        "        count2 = 0\n",
        "        while antonym == word and count2 != len(antonyms):\n",
        "            antonyms.remove(word)\n",
        "            antonym = random.choice(antonyms)\n",
        "            count2 += 1\n",
        "\n",
        "        augmented_hyp = augmented_hyp.replace(word, antonym)  # Updated this line to replace in the string\n",
        "\n",
        "        if augmented_labels == 'ENTAILMENT':\n",
        "            augmented_labels = 'CONTRADICTION'\n",
        "        elif augmented_labels == 'CONTRADICTION':\n",
        "            augmented_labels = 'ENTAILMENT'\n",
        "        else:\n",
        "            augmented_labels ='NEUTRAL'\n",
        "\n",
        "    else:\n",
        "        synonyms=get_synonyms(word, tag)\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            count2=0\n",
        "            while synonym==word and count2!=len(synonyms):\n",
        "                synonyms.remove(word)\n",
        "                synonym = random.choice(synonyms)\n",
        "                count2+=1\n",
        "\n",
        "            # synonym=synonyms[0]\n",
        "            augmented_hyp = augmented_hyp.replace(word, synonym)\n",
        "\n",
        "\n",
        "    return augmented_hyp, augmented_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzjvAWDaBYcS"
      },
      "source": [
        "<h3>Back Translation<h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMt2J6atBYcS"
      },
      "source": [
        "I used cross-lingual augmentation technique, involving back-translation, which proved to be the most sophisticated method in preserving the original sentence's semantics, but at the cost of increased processing time.\n",
        "Its a bit time consuming it takes around 23 minute to back-translate 1000 sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p04iE2WFBYcT"
      },
      "outputs": [],
      "source": [
        "# Function to perform back translation with retry logic\n",
        "def Cross_lingual_augmentation(text, src_lang='en', mid_lang='it', retries=3):\n",
        "    translator = Translator()\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Translate text to intermediate language\n",
        "            translated = translator.translate(text, src=src_lang, dest=mid_lang).text\n",
        "            # Translate back to the original language\n",
        "            back_translated = translator.translate(translated, src=mid_lang, dest=src_lang).text\n",
        "            return back_translated\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}, retrying back translation... ({attempt + 1}/{retries}) for text: {text}\")\n",
        "            time.sleep(1)  # Wait before retrying\n",
        "    return text  # Return original text if all retries fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzRMjapMBYcU"
      },
      "source": [
        "<h3>Noise_Insertion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pItbFL1FBYcU"
      },
      "source": [
        "I used Noise insertion technique which simply find a random words or letter in each sentence and add it again in a sentence. This technique can help the model to be more robust to any typos that may be present in a data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Wf2qbjBYcV"
      },
      "outputs": [],
      "source": [
        "def random_insertion(sentence):\n",
        "    n=1\n",
        "    words = sentence.split()\n",
        "    for _ in range(n):\n",
        "        new_word = random.choice(words)\n",
        "        position = random.randint(0, len(words))\n",
        "        words.insert(position, new_word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def noise_injection(sentence):\n",
        "    noise_level=0.07\n",
        "    words = list(sentence)\n",
        "    num_noises = int(len(words) * noise_level)\n",
        "    for _ in range(num_noises):\n",
        "        pos = random.randint(0, len(words) - 1)\n",
        "        words[pos] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "    return ''.join(words)\n",
        "\n",
        "\n",
        "# Function to apply back translation on hypothesis\n",
        "def Noise_augmentation(hypothesis):\n",
        "    techniques = [\"word\",  \"letter\"]\n",
        "    technique = random.choice(techniques)\n",
        "    if technique == \"word\":\n",
        "        augmented_sentence=random_insertion(hypothesis)\n",
        "    elif technique == \"letter\":\n",
        "        augmented_sentence=noise_injection(hypothesis)\n",
        "\n",
        "    return augmented_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZehEc3emBYcW"
      },
      "source": [
        "<h3>Words Swapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpliAXzBYcW"
      },
      "source": [
        "The technique word-swap, where subjects or objects within a sentence were exchanged, resulting in slight disorganization without losing semantic meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT8GYJ4xBYcX"
      },
      "outputs": [],
      "source": [
        "# Function to get synonyms from WordNet\n",
        "def get_synonyms_wo_pos(word):\n",
        "    synonyms = set()\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().replace('_', ' '))\n",
        "    return list(synonyms)\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to swap two words in a sentence\n",
        "def swap_augmentaion(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Identify words to swap (e.g., subject and object)\n",
        "    words_to_swap = []\n",
        "    for token in doc:\n",
        "        if token.dep_ in {\"nsubj\", \"dobj\"}:  #\n",
        "            words_to_swap.append((token.text, token.i))\n",
        "        # if len(words_to_swap) == 2:\n",
        "        #     break\n",
        "    if len(words_to_swap) >= 2:\n",
        "        random_words_to_swap = random.sample(words_to_swap, 2)\n",
        "    # If less than two words are found, no swapping is possible\n",
        "    if len(words_to_swap) == 0:\n",
        "        return sentence\n",
        "\n",
        "    if len(words_to_swap) < 2:\n",
        "        # synonyms=get_synonyms_wo_pos(words_to_swap[0][0])\n",
        "\n",
        "        # if synonyms:\n",
        "        #     synonym = random.choice(synonyms)\n",
        "        #     count2=0\n",
        "        #     while synonym==words_to_swap[0][0]and count2!=len(synonyms):\n",
        "        #         # synonyms.remove(words_to_swap[0][0])\n",
        "        #         synonym = random.choice(synonyms)\n",
        "        #         count2+=1\n",
        "\n",
        "        #     # synonym=synonyms[0]\n",
        "        #     sentence = sentence.replace(words_to_swap[0][0], synonym)\n",
        "        return sentence\n",
        "\n",
        "    # Swap the positions of the identified words\n",
        "    word1, index1 = random_words_to_swap[0]\n",
        "    word2, index2 = random_words_to_swap[1]\n",
        "\n",
        "    # Convert doc to list of tokens\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # Swap the words in the token list\n",
        "    tokens[index1], tokens[index2] = tokens[index2], tokens[index1]\n",
        "\n",
        "    # Join the tokens back into a sentence\n",
        "    swapped_sentence = \" \".join(tokens)\n",
        "    return swapped_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69oxgD-CBYcX"
      },
      "source": [
        "Each technique was assigned a specific percentage to determine its application across the subset of 15,000 samples, The techniques assigned a lower percentage were chosen as such due to their higher likelihood of causing a loss of semantic meaning in the sentence. The augmenting took around 3 hours to process for 15000 samples, which were than combine and shuffled across the original training dataset  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz2R4v5XBYcY",
        "outputId": "1098a369-9267-4ab7-de07-a18d8aeaedb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'synonym': 7500, 'antonym': 1500, 'swap': 1500, 'noise': 1500, 'back_translation': 3000}\n"
          ]
        }
      ],
      "source": [
        "augmentation_percentages = {\n",
        "    \"synonym\": 0.5,\n",
        "    \"antonym\": 0.1,\n",
        "    \"swap\": 0.1,\n",
        "    \"noise\": 0.1,\n",
        "    \"back_translation\": 0.2,\n",
        "}\n",
        "def calculate_samples_per_technique(dataset_size, percentages):\n",
        "    num_samples = {}\n",
        "    for technique, percentage in percentages.items():\n",
        "        num_samples[technique] = int(dataset_size * percentage)\n",
        "    return num_samples\n",
        "\n",
        "# Example usage\n",
        "dataset_size = len(subset_dataset)\n",
        "num_samples = calculate_samples_per_technique(dataset_size, augmentation_percentages)\n",
        "print(num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdxUY_mUBYcZ",
        "outputId": "005c230a-860b-4398-a708-054ba53d4602"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
              "    num_rows: 15000\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PzW2r81BYca"
      },
      "outputs": [],
      "source": [
        "def augment_samples(example):\n",
        "    used=[]# to see which technique was used during each sample to augment\n",
        "    dataset_size = len(example)\n",
        "    num_samples = calculate_samples_per_technique(dataset_size, augmentation_percentages)\n",
        "    # num_samples\n",
        "    aug_count={'synonym': 0, 'antonym': 0, 'swap': 0, 'noise': 0, 'back_translation': 0}\n",
        "    techniques = [\"synonym\",\"antonym\",\"swap\",\"noise\",\"back_translation\"]\n",
        "\n",
        "    augmented_hyp = example['hypothesis'][:]\n",
        "    augmented_labels= example['label'][:]\n",
        "    wsd = example['wsd']\n",
        "    wsd_annotations = []\n",
        "\n",
        "    for i in range(len(wsd)):\n",
        "        extracted_hyp = wsd[i]['hypothesis']\n",
        "        wsd_annotations.append(extracted_hyp)\n",
        "\n",
        "    for i in tqdm(range(len(example['hypothesis'])), desc=\"Augmenting\"):\n",
        "        repeat=True\n",
        "        attempt=len(example['hypothesis'][i])\n",
        "        while repeat and attempt !=0:\n",
        "            technique=random.choice(techniques)\n",
        "            if technique == 'synonym' and aug_count['synonym']< num_samples[\"synonym\"]:\n",
        "                new_hyp= apply_synonym_substitution(example['hypothesis'][i], wsd_annotations[i])\n",
        "                attempt-=1\n",
        "                if new_hyp!=augmented_hyp[i]:\n",
        "                    used.append(technique)\n",
        "                    augmented_hyp[i]=new_hyp\n",
        "                    repeat=False\n",
        "                    aug_count['synonym']+=1\n",
        "\n",
        "            elif technique == 'antonym' and aug_count['antonym'] < num_samples[\"antonym\"]:\n",
        "                new_hyp, new_label= apply_antonym_substitution(example['hypothesis'][i], wsd_annotations[i],augmented_labels[i])\n",
        "                attempt-=1\n",
        "                if new_hyp!=augmented_hyp[i]:\n",
        "                    used.append(technique)\n",
        "                    augmented_hyp[i],augmented_labels[i]=new_hyp, new_label\n",
        "                    repeat=False\n",
        "                    aug_count['antonym']+=1\n",
        "\n",
        "            elif technique == 'swap' and aug_count['swap'] < num_samples[\"swap\"]:\n",
        "                new_hyp = swap_augmentaion(example['hypothesis'][i])\n",
        "                attempt-=1\n",
        "                if new_hyp!=augmented_hyp[i]:\n",
        "                    used.append(technique)\n",
        "                    augmented_hyp[i]=new_hyp\n",
        "                    repeat=False\n",
        "                    aug_count['swap']+=1\n",
        "                if attempt==0 and new_hyp==augmented_hyp[i]:# if we cant find words to swap after many attempts than apply synonym substitution\n",
        "                    new_hyp= apply_synonym_substitution(example['hypothesis'][i], wsd_annotations[i])\n",
        "                    augmented_hyp[i]=new_hyp\n",
        "                    aug_count['swap']+=1\n",
        "\n",
        "\n",
        "            elif technique == 'noise' and aug_count['noise'] < num_samples[\"noise\"]:\n",
        "                new_hyp = Noise_augmentation(example['hypothesis'][i])\n",
        "                attempt-=1\n",
        "                if new_hyp!=augmented_hyp[i]:\n",
        "                    used.append(technique)\n",
        "                    augmented_hyp[i]=new_hyp\n",
        "                    repeat=False\n",
        "                    aug_count['noise']+=1\n",
        "\n",
        "\n",
        "            elif technique == 'back_translation' and aug_count['back_translation'] < num_samples[\"back_translation\"]:\n",
        "                new_hyp = Cross_lingual_augmentation(example['hypothesis'][i])\n",
        "                # attempt==0\n",
        "                used.append(technique)\n",
        "                if new_hyp==augmented_hyp[i]:# if the traslated version is same as orignal than apply synonym substitution\n",
        "                   new_hyp= apply_synonym_substitution(example['hypothesis'][i], wsd_annotations[i])\n",
        "\n",
        "                augmented_hyp[i]=new_hyp\n",
        "                repeat=False\n",
        "                aug_count['back_translation']+=1\n",
        "\n",
        "\n",
        "            if attempt==0:\n",
        "                used.append('Same as original')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return augmented_hyp,augmented_labels, used\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPBWW5TBBYcb",
        "outputId": "69ea532b-c810-4f87-8cca-adad9025c280"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augmenting: 100%|██████████| 15000/15000 [1:17:09<00:00,  3.24it/s] \n"
          ]
        }
      ],
      "source": [
        "a_h, a_l, a_t=augment_samples(subset_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf8jtaaOBYcb"
      },
      "outputs": [],
      "source": [
        "# Ensure the length of the new hypothesis list matches the number of rows in the dataset\n",
        "assert len(a_h) == subset_dataset.num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wFnGvABYcc"
      },
      "outputs": [],
      "source": [
        "# Replace the hypothesis column\n",
        "def replace_hypothesis(example, idx):\n",
        "    example['hypothesis'] = a_h[idx]\n",
        "    example['label'] = a_l[idx]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MvqgT8NBYcc"
      },
      "outputs": [],
      "source": [
        "# aug_dataset = subset_dataset.map(replace_hypothesis, with_indices=True)\n",
        "aug_dataset = subset_dataset.map(replace_hypothesis, with_indices=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVscHErWBYcd",
        "outputId": "f8c855d1-d13d-4e96-b4d5-d49f3b3b7baf",
        "colab": {
          "referenced_widgets": [
            "bcce1598d6a94f4dbf60abfb4692a8b1"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcce1598d6a94f4dbf60abfb4692a8b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/15000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "aug_dataset.save_to_disk('adverserial_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAEC8_TkBYce"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "Adverserial_Dataset = load_from_disk('adverserial_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeS-IyCbBYcf",
        "outputId": "8282d9df-1d45-4ebd-9068-5b35a560ff09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': Value(dtype='string', id=None), 'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'wsd': {'premise': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}], 'hypothesis': [{'index': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'lemma': Value(dtype='string', id=None), 'bnSynsetId': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None)}]}, 'srl': {'premise': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}, 'hypothesis': {'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}], 'annotations': [{'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}]}}}\n",
            "{'id': Value(dtype='string', id=None), 'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), 'wsd': {'hypothesis': [{'bnSynsetId': Value(dtype='string', id=None), 'index': Value(dtype='int64', id=None), 'lemma': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None)}], 'premise': [{'bnSynsetId': Value(dtype='string', id=None), 'index': Value(dtype='int64', id=None), 'lemma': Value(dtype='string', id=None), 'nltkSynset': Value(dtype='string', id=None), 'pos': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'wnSynsetOffset': Value(dtype='string', id=None)}]}, 'srl': {'hypothesis': {'annotations': [{'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}], 'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}]}, 'premise': {'annotations': [{'englishPropbank': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}, 'tokenIndex': Value(dtype='int64', id=None), 'verbatlas': {'frameName': Value(dtype='string', id=None), 'roles': [{'role': Value(dtype='string', id=None), 'score': Value(dtype='float64', id=None), 'span': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}]}}], 'tokens': [{'index': Value(dtype='int64', id=None), 'rawText': Value(dtype='string', id=None)}]}}}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'].features)\n",
        "print(Adverserial_Dataset.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvd6kasIBYcg"
      },
      "source": [
        "Combining and shuffling with original training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cAGSemiBYch"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "# Assuming you have already loaded your datasets, e.g., first_dataset and second_dataset\n",
        "\n",
        "# Combine the train set of the first dataset with the second dataset\n",
        "combined_train_dataset = concatenate_datasets([dataset['train'], Adverserial_Dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp9SqAv2BYci"
      },
      "outputs": [],
      "source": [
        "# Shuffle the combined dataset\n",
        "shuffled_combined_train_dataset = combined_train_dataset.shuffle(seed=42)\n",
        "\n",
        "# Replace the train set in the first dataset with the shuffled combined dataset\n",
        "combined_dataset_dict = DatasetDict({\n",
        "    'train': shuffled_combined_train_dataset,\n",
        "    'validation': dataset['validation'],\n",
        "    'test': dataset['test']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA3o0XkVBYcj",
        "outputId": "26dbd530-59a0-47cc-aa2d-a7fabd1004a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 66086\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 2288\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
            "        num_rows: 2287\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Print to verify\n",
        "print(combined_dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVqVcY36BYck",
        "outputId": "fd4ee78f-5633-438a-a0d7-d9664deec012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auggmented hypothesis: Michael Fassbender is an worker., Label: ENTAILMENT, Technique used: synonym, length:32 \n",
            "original hypothesis:   Michael Fassbender is an actor., Label: ENTAILMENT, length:31\n"
          ]
        }
      ],
      "source": [
        "ran= random.randint(0,499)\n",
        "print(f\"auggmented hypothesis: {aug_dataset[ran]['hypothesis']}, Label: {aug_dataset[ran]['label']}, Technique used: {a_t[ran]}, length:{len(aug_dataset[ran]['hypothesis'])} \")\n",
        "print(f\"original hypothesis:   {subset_dataset[ran]['hypothesis']}, Label: {subset_dataset[ran]['label']}, length:{len(subset_dataset[ran]['hypothesis'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZVWoHvuBYck"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_9jbbgQBYdD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}